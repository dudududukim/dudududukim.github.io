<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> d2f (digital 2 film) | Duhyeon Kim </title> <meta name="author" content="Duhyeon Kim"> <meta name="description" content="deep learning-based film grain and color synthesis"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%89&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dudududukim.github.io/projects/d2f/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Duhyeon</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">hobbies </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">d2f (digital 2 film)</h1> <p class="post-description">deep learning-based film grain and color synthesis</p> </header> <article> <blockquote> <p>📆 25.01.31 ~ Temporary suspension</p> </blockquote> <h4 id="-tech-stack">🛠 Tech Stack</h4> <ul> <li>mlx framework</li> <li>python / pytorch</li> <li>etc…</li> </ul> <hr> <h3 id="-why-start-this">🎯 Why Start This?</h3> <p>Since I’m unsure about what I want to be—a businessman, a startup member, a personal developer, or something else—I’ve been feeling uneasy these days. However, one thing I’ve realized is that I have a strong desire to solve problems or fulfill my own needs by learning new skills, such as AI, circuit design, and system development. Whether intentional or not, I have a day off today, so I’m setting aside my future plans and immersing myself in this project.</p> <p><strong>Final goal</strong> is :</p> <ol> <li>making active users with my product</li> <li>finishing with a level of completeness that makes it presentable</li> </ol> <p>Praying for myself 🧯</p> <hr> <h3 id="-why-suspension">😭 Why suspension?</h3> <ol> <li> <p><code class="language-plaintext highlighter-rouge">Resource limitration</code> -&gt; i can’t run EfficientDet(which is purposed for efficiency, using the feature fusion tech.) or ViT <br></p> </li> <li> <p>The max capa of my M2 air 16gb with proper batch size is UNET-512 <br></p> </li> <li> <p>The film grain and colour complexity is not that easy, i thought as i have the my own data i can make it but unlike other tasks that deals with a specific purpose, <code class="language-plaintext highlighter-rouge">considering 2 factors in one pipeline</code> is complex. <br></p> </li> <li> <p><code class="language-plaintext highlighter-rouge">Deployment is not my style</code>. If the product is all made, i could tried but without any motivation it was hard for me to learn the AWS tools. <br></p> </li> <li> <p>Also, thought <code class="language-plaintext highlighter-rouge">data</code> is enough (375 pics) since it has high resolutions(2048x3089). but as we need to use it with resized data to fit in memory and make model capture the overall colour. It <code class="language-plaintext highlighter-rouge">was not enough</code>.</p> </li> </ol> <hr> <h3 id="-what-i-have-learned">📗 What i have learned?</h3> <ol> <li> <p><a href="https://github.com/zhengdihan/Unsupervised_denoising" rel="external nofollow noopener" target="_blank">Unsupervised Denosier</a>를 읽으면서 model 자체를 하나의 이미지의 denosing을 위해서 사용한다는 점이 놀라웠다. 대부분은 모델을 학습하고 freeze된 상태에서의 결과물 추론을 기대하는데, 이 논문에서는 <code class="language-plaintext highlighter-rouge">모델을 하나의 재귀함수처럼</code> 사용하여 prior를 따라가도록 만드는 과정이 너무나 신기했고, 이러한 개념을 이해하기 위해서 <code class="language-plaintext highlighter-rouge">확률이 얼마나 중요한지</code>를 알 수 있었다. 특히 AMDD이라는 방법을 완벽하게 이해하지는 못했지만, 노이즈 이미지 만을 가지고 디노이징을 해나악는 과정에서의 퀄리티도 좋았고, 모델을 바라보는 새로운 관점을 느낄 수 있었다. <br></p> </li> <li> <p>pytorch의 학습에 대한(vision model 중심) convention을 많이 익힐 수 있었다. 특히 UEGAN은 <code class="language-plaintext highlighter-rouge">IEEE T-IP</code> 에 개시된 논문으로 CVPR, ECCV와 같은 속도감 있는 연구와 달리 연구의 완성도를 느낄 수 있었으며, 해당 <a href="https://github.com/eezkni/UEGAN/tree/master" rel="external nofollow noopener" target="_blank">source code</a>를 읽어보면서 pytorch 학습의 디테일한 부분들을 학습할 수 있었다. 특히 IEEE T-IP 특성인지 코드가 굉장히 깔끔하게 정리되어있었고, <code class="language-plaintext highlighter-rouge">깔끔한 코드가 가독성에 얼마나 도움이 많이되는지</code>를 알 수 있었다. 이전까지는 코드를 보면서 이부분도 중요한 역할인가에 대한 직관이 없었다면, UEGAN 뿐만 아니라 다양한 pytorch code implementation을 보면서 어떤 부분을 집중적으로 봐야하고, 어떤 부분은 convention적인지 구분하는 직관을 가지게 되었다. <br></p> </li> <li> <p>model training에서 <code class="language-plaintext highlighter-rouge">메모리와 computation의 효율성</code>과 capacity의 중요성을 느낄 수 있었다. 특히 더 성능 좋은 모델을 위해서 가중치를 높이면 가중치 갯수가 증가하는 것도 문제지만, 해당 가중치의 grad를 추적하기 위해서 사용되는 메모리량도 늘어나기 때문에 얼마나 성능이 메모리 집약적인지를 느낄 수 있었다. 또 모델의 computation 능력에 비해서 <code class="language-plaintext highlighter-rouge">memory w/r 비율이 높으면 성능 비효율</code>이 발생하기 때문에 prefetcher와 같은 데이터 로더를 사용하거나, tqdm과 time으로 학습 초기에는 compute / read 효율을 측정하는 것도 좋은 profiling 방법이라는 점을 알게되었다. <br></p> </li> <li> <p>하나의 목표를 향해서 필요한 자료들과 논문들을 읽으면서 공부를 해나가는 과정은 확실히 흥미롭다는 점을 알게되었고, 진정한 전문가가 되기 위해서 읽어야할 논문들이 얼마나 많은 가를 알게되었다. 어떠한 목표를 위해서는 <strong>그 목표로 작성된 논문 뿐만 아니라 해당 분야 자체의 다양한 논문을 읽는 것</strong>이 훨씬 더 다양한 접근을 가능하게 할 것이라는 확신이 들었다. 비단 CV라도 NLP의 attention mechanism을 분류모델에서는 적극활용하는 점은 AI라는 분야 전반에 대한 관심이 필요함을 알 수 있다. 그리고 공부를 driving하는 force는 그 분야에 대한 관심일 수도 있지만, 내가 만들고자 하는 하나의 가치를 명확하게 하는 것이 훨씬더 중요하다고 생각하게 되었다. 그런 의미에서의 나의 분야에 대한 나의 과감한 선택이 나를 기다리고 있다고 생각한다. <br></p> </li> <li> <p>또 ViT를 공부하면서, deepseek의 multi-head latent attention같은 경우에도 결국 latent space 개념을 활용하는 건데, 이는 VAE에서도 나오는 개념이니 task과 관련없이 중요한 <strong>milestone과 같은 개념들을 익히고 고정관념을 피하려고 하는 노력</strong>이 필요할 것 같다. <br></p> </li> <li> <p>그리고 분명히 AI의 시대가 도래했지만, 여전히 <code class="language-plaintext highlighter-rouge">legacy algorithm의 중요성</code>과 효율성을 무시해서는 안된다는 점을 알게 되었다. ADMM을 위해서 사용된 BM3D나 grain noise detect을 위해서 사용된 Homogeniety block detection 과 같은 윈도우 기반의 방법들은 결국 내가 지금 가진 자원내에서 내가 필요한 성능을 위해서 적합한 방식을 모색할때 더 넓은 시야를 가지게 해줄 것이다. <br></p> </li> <li> <p><code class="language-plaintext highlighter-rouge">mlx</code>와 같은 하드웨어 최적화 프로그램이나 framework를 위해서는 결국 메모리를 직접적으로 관리하는 <strong>low level programming language의 필요성</strong>을 더욱 체감할 수 있었다. 내가 python을 사용하는 사람이 되더라도 필요에 따라서 source code 자체를 수정하고 build할 수 있는 사람이 되고 싶다. <br></p> </li> <li> <p>또한 현재의 cpu, gpu, tpu와 같은 Heterogeneous computing system에서 apple silicon과 같은 <code class="language-plaintext highlighter-rouge">Unified Memory</code>가 가지는 이점을 확실히 알 수 있었다. 특히 CUDA의 경우 pin_memory option을 사용하는데 이는 운영체제에서 swap 못하는 page-locked memory에 바로 dataloader가 data를 올림으로써 DMA가 바로 GPU로 데이터를 복사할 수 있도록 한다. pin_memory=false일시, <strong>pageable memory에서 page-locked memory로 이동하는 추가과정</strong>이 필요하기 때무네 GPU와 CPU간 데이터 전송에 병목이 발생할 수 있는 것이다. 이를 Unified memory의 경우는 GPU CPU가 메모리 공간을 공유하기 때문에 불필요한 데이터 이동이 없다. 하지만 이는 CPU GPU가 제한된 노트북과 같은 환경에서는 유용할 수 있지만, 대규모 서버 시스템에서는 확장성에서 GPU자체의 메모리가 존재하는게 훨씬 효율적일 것으로 생각된다. 이와 같이 결국 SW를 다루더라도 하드웨어에 대한 지식 전문성을 다르게 만든다는 확신이 든다. 그럼에도 NVIDIA의 연산성능은 정말 뛰어나다. <br></p> </li> <li> <p>그리고 무엇보다 내용의 전문성이 높아진다면 꼭! <code class="language-plaintext highlighter-rouge">공식문서를 정독하는 것이 중요</code>하다는 것을 느꼈다. 아무리 GPT랑 대화하더라도 결국 나의 환경에서의 변수가 있고, 공식문서를 읽게되면 gpt가 알려주지 않는 다양한 기능들을 더 알 수 있다. 초기에는 빠르게 gpt로 접근하더라도 조금 복잡하거나 모호한 개념에 대해서는 공식문서에 시간을 투자하는 과감한 선택은 필수라고 생각한다.</p> </li> </ol> <p><br></p> <hr> <h2 id="thigns-done-️">Thigns done ⬇️</h2> <p><br></p> <h3 id="-mlx-framework">📝 MLX framework</h3> <hr> <p><em>1.</em></p> <h4 id="mlxdatabuffer_from_vector"><em>mlx.data.buffer_from_vector</em></h4> <p><strong>error</strong> : dictionary로 구성된 sample = [{‘image’ : b’Path/’}],</p> <p>이거를 buffer_from_vector를 mlx.data로 실행하면 byte array가 넘어와야하는데, 빈배열이 넘어옴 ([] dtype=int8)</p> <p><strong>Solution</strong> :</p> <p>Mac 초기화 -&gt; mlx-data를 pip로 install하지 않고 소스코드에서 python blinding으로 설치함</p> <p>(환경변수와 다양한 dependency가 꼬여있던 것으로 파단됨)</p> <hr> <p><em>2.</em></p> <h4 id="mlx-framework-insights"><em>mlx framework insights</em></h4> <p><strong>Buffer</strong> : image를 필요시에 load하여 불필요하게 load되는 경우를 막는다. -&gt; 애플이 지리긴하는듯 (결국 Apple silicon을 만든이상 그들의 framework와 protocol 개발은 불가피하고, 여기서도 만약에 두각을 드러낸다면 결국 애플 사이클 다시 온다고 봄)</p> <p><strong>stream.prefetch(4, 4)</strong> : CPU에서 4개의 batch를 GPU연산동안 load해두고, GPU도 4개의 batch를 한번에 불러드리도록 설정함</p> <hr> <p><em>3.</em></p> <h4 id="binary-cross-entropy에서-mxcompile-문제"><em>binary cross entropy에서 mx.compile 문제</em></h4> <p>해결 못함 ➡️ 그냥 num_classes=2 로 하고 mlx.nn.losses.cross_entropy로 함</p> <p>binary cross entropy에서 @partial(mx.compile) 옵션에서 문제가 발생하는 듯함</p> <hr> <p><em>4.</em></p> <h4 id="training-overfitting-resent44"><em>Training overfitting (resent44)</em></h4> <p>training data가 204개에서 training이 overfitting이 발견하는 것을 확인하였다.</p> <p>데이터는 (224,224) center crop되어 사용되었고, batch_size = 16, epoch=50, lr=1e-5</p> <table> <thead> <tr> <th>Epoch</th> <th>Train Loss</th> <th>Train Accuracy</th> <th>Test Accuracy</th> </tr> </thead> <tbody> <tr> <td>49</td> <td>0.040</td> <td>0.990</td> <td>0.688</td> </tr> </tbody> </table> <p>📙 이유? 아마도 data가 부족한듯 하다. image crop까지는 어떻게 하긴하는데, crop보다는 224,224 데이터를 새로 생성해서 진행해야겠다.</p> <hr> <p><em>5.</em></p> <h4 id="224224로-이미지-쪼개서-학습-진행"><em>224,224로 이미지 쪼개서 학습 진행</em></h4> <p>💾 원래 데이터 : 1.02GB -&gt; 743MB (6.13GB disk)</p> <p>train length : 42111<br> Test length : 4678</p> <p>🙋🏾‍♂️ 했는데도 training acc는 1에 epoch1만에 수렴하는데, test accur는 서서히 올라가긴하나 <strong>overfitting을</strong> 보임</p> <p>사실 내가 어떤 프로젝트를 진행하거나 공부를 진행할때 항상 이러한 넘기 귀찮은 문제들에서 포기하고 그냥 학교나 사회에서 주어진 것들을 열심히 하기 위해서 돌아갔었다. 그런데 이번 한달의 목표를 이러한 나를 마주하고 그러한 허들을 침착고 조용히 꾸준하게 넘어가려고 한다. 이번에도 사실 그냥 ‘아 뭐야 몰라’하고 넘어갈 수 있지만 <strong>나는 이번만큼은 끝을 봐야한다.</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/unbalanced_dataset-480.webp 480w,/assets/img/d2f/unbalanced_dataset-800.webp 800w,/assets/img/d2f/unbalanced_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/unbalanced_dataset.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>보면, buffer를 랜덤하게 shuffle하고 plot을 해보았는데, <strong>데이터의 상당한 편향</strong>을 발견했다. 224x224는 동일하게 crop했지만 1.7x 좋아서 croppd에서 3:1 정도로 image가 많은 것을 확인했다. 데이터의 밸런스를 맞춰서 다시 학습을 해보아야겠다.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/balanced_dataset-480.webp 480w,/assets/img/d2f/balanced_dataset-800.webp 800w,/assets/img/d2f/balanced_dataset-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/balanced_dataset.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>sample load하고 spilt하는 코드에 min_count로 가장 갯수가 적은 label에 대해서 1:1로 data balance를 맞출 수 있게 하였다.<br> +optimizer를 adma에서 sgd 0.01 momentum 0.9 weight_decay=5e-4로 하고, 50step 마다 opt update하도록 수정</p> <p>📖 module.load_weights로 .npz load를 하고 unseen data에 대해서 적용해본 결과 거의 모든 경우에서 올바른 예측을 하였다.<br> 하지만, 해당 결과를 백트래킹해보니, wiener2 7x7 필터만 적용해도 Discriminator를 전부 속이는 것이 가능했다. + noise를 추가해도 안됨<br> 따라서 데이터를 조금 더 가공해서 다시 학습을 시켜야 겠다. 그리고 접근법을 여러가지의 chain으로 가져가는 것이 맞을 것 같다는 생각이든다.</p> <p><br></p> <h3 id="-denoiser-appoach">🧹 Denoiser Appoach</h3> <hr> <p><em>6.</em></p> <h4 id="dncnn-도입"><em>dncnn 도입</em></h4> <p>🖋️ 현재 resnet20 기반의 d2f_D(discriminator)가 필름의 색감을 학습했다기 보다는 texture를 학습했다고 생각된다.<br> DnCnn을 기반으로 denoising한 결과로 inference를 해본 결과가 아래와 같다. (완벽히 Discriminator를 속임)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/model_20_32_missed-480.webp 480w,/assets/img/d2f/model_20_32_missed-800.webp 800w,/assets/img/d2f/model_20_32_missed-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/model_20_32_missed.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>따라서 지금 현재의 dataset을 두개로 분리<br> ☝️ DnCnn 적용된 set(동일하게 적용) -&gt; 한쪽만 적용시 또 dncnn의 특성을 파악할것 같다는 판단<br> 💕 기존 원본 이미지</p> <p><br>↦ 그리고 지금까지의 모델이 discriminator로써 행동하는 모습을 보았을때, 학습의 안정성을 위해서는 texture와 색감을 동시에 진행하는 게 아니라 <br> 2가지 모듈의 pipeline이 필요해 보인다.</p> <hr> <p><em>7.</em></p> <h4 id="dncnn-applied-dataset-d2f_d-결과"><em>dncnn applied dataset d2f_D 결과</em></h4> <p>dncnnd을 통과시킨다고, grain 특성이 사라지지는 않음 ➡️ 기존의 이미지의 특성과 전혀 다른 특성으로 보여짐</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/dncnn_img-480.webp 480w,/assets/img/d2f/dncnn_img-800.webp 800w,/assets/img/d2f/dncnn_img-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/dncnn_img.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <blockquote> <h4 id="torch로-변경">Torch로 변경</h4> <p>💡 MLX에서의 performance가 dramatic하지 않고, MVP를 빠르게 만드는것을 목표로 변경</p> </blockquote> <hr> <p><em>8.</em></p> <h4 id="grain-synthesis-weakness"><em>Grain Synthesis Weakness</em></h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>FFT High Frequency Energy Ratios:
Digital Image: 28.44%
Film Image: 83.57%
Difference (Film - Digital): 55.13%
</code></pre></div></div> <p>해당 결과를 이용해서 torch.fft기반의 disciriminator 성분을 만들어야할 것 같다.</p> <p>별다른 성과는 없음 ➡️ fft를 discriminator에만 추가하는 방식은 generator가 high frequency 성분을 학습하게 하는데 한계가 있다.<br> 또한 현재 image의 학습에서의 사이즈가 256x256인데, film 이미지의 bilinear transform에서 많은 texture 정보가 날아가는 것을 확인할 수 있다.<br></p> <p>따라서 현재 UEGAN에서는 색감을 학습시키는 방향으로 하고, 필름 이미지를 denosiing하여서 paired data로 film grain model을 추가 학습하는 방향으로 가려고 한다.</p> <hr> <p><em>9.</em></p> <h4 id="denoising-networks"><em>denoising Networks</em></h4> <p>Denoising이나 image resolution등과 같은 모델을 볼 때, 좋은 사이트를 찾음</p> <p><a href="https://paperswithcode.com/task/style-transfer" rel="external nofollow noopener" target="_blank">paperswithcode</a></p> <p>Dataset을 검색하면 해당 데이터셋을 활용하는 task의 논문들이 정리되어 있어서 SOTA나 나와 가장 fit한 모델을 찾을 수 있었다.<br> 나는 지금 KODAK24 dataset을 활용하는 모델들 위주로 확인한다.</p> <p>📝 : Restormer / SwinIR / DMID-d</p> <p>DMID(diffusion based) : <a href="https://github.com/li-tong-621/dmid?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">Github DMID</a><br> Restormer(transformer based) : <a href="https://github.com/swz30/restormer?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">Github Restormer</a></p> <hr> <p><em>10.</em></p> <h4 id="unsuperviesd-denoiser"><em>Unsuperviesd Denoiser</em></h4> <p>이게 제일 대박이라고 생각됨</p> <p>Stimulating Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling<br> 에서 사용되는 부분적인 gaussian noiser 였는데, 대박임</p> <p>📝 (Insane) AN UNSUPERVISED DEEP LEARNING APPROACH FOR REAL-WORLD IMAGE DENOISING</p> <p>내가 생각하는 AI의 방향임 / 자체적으로 학습을 이어서 하다가 ➡️ 이정도면 됐다고 할때 그만둠</p> <p>Key idea</p> <ul> <li>SURE (Stein’s Unbiased Risk Estimator)</li> <li>Unet Based Enc / Dec -&gt; learning picture by picture gaussian denoising</li> </ul> <p>이미지 마다 DL을 학습해야되서 cost는 높지만, 이러한 방식이 합쳐지면 굉장히 좋은 시너지를 낼 수 있다고 생각함</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/Unet_denoiser-480.webp 480w,/assets/img/d2f/Unet_denoiser-800.webp 800w,/assets/img/d2f/Unet_denoiser-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/Unet_denoiser.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>‼ 결국 모든 현상은 Normal Distribution을 따르게 되니까</p> <p>😉 문제는 이미지당 50분이 걸린다는거임</p> <p>읽다가 포기함 / lagrangian 까지는 어떻게 이해하겠는데, ADMM이랑 augmented lagrangian method의 수식을 이해하기 어려움</p> <p>최적화 이론 수업을 통해서 해결해야되는 문제</p> <hr> <p><em>11.</em></p> <h4 id="neural-styletrasnfer"><em>Neural Styletrasnfer</em></h4> <p>일단 색감 정보는 UEGAN의 구조와 loss function이 잘 지켜준다고 생각했을때, texture에 대한 내용을 어떻게 해결할지에 집중</p> <p>style transfer의 근본 논문에서부터 vgg19의 usage를 확인해보려고함</p> <p>👍 Gatys가 사용한 gram matrix에서 vgg19의 conv block 별 content style 정보를 가지는 layer를 구분하고 있는 사실을 발견함 ➡️ 이를 활용하여서 비슷한 content의 다른 style인 사진을 그나마 골라서 비교해보려고 함</p> <ul> <li>초반 레이어(conv1~conv3): 너무 로우레벨(low-level) 특징 (에지, 질감) → content 정보가 충분하지 않음.</li> <li>중간 레이어(conv4_2, conv5_2): 형태(structure)와 의미적인 정보가 잘 유지됨 → content feature로 적합.</li> <li>깊은 레이어(conv5_4 이후): 추상적인 개념이 강해져서 세부 구조 손실 가능성 → content 정보로 부적합.</li> </ul> <p>☕️ vgg layer 별 cosine 유사도(film image vs digital image)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/vgg_gram_matrix-480.webp 480w,/assets/img/d2f/vgg_gram_matrix-800.webp 800w,/assets/img/d2f/vgg_gram_matrix-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/vgg_gram_matrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>☕️ vgg layer 별 cosine 유사도(digital image vs digital image with GN)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/vgg_AWGN-480.webp 480w,/assets/img/d2f/vgg_AWGN-800.webp 800w,/assets/img/d2f/vgg_AWGN-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/vgg_AWGN.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>확실히 vgg의 content feature와 style feature가 존재하기는 한다고 생각함 (content loss와 style loss weight를 조절하는 것도 중요한 요소임)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/NST-480.webp 480w,/assets/img/d2f/NST-800.webp 800w,/assets/img/d2f/NST-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/NST.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/kian-480.webp 480w,/assets/img/d2f/kian-800.webp 800w,/assets/img/d2f/kian-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/kian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>다양한 방식을 시도해본 결과, 이건 확실히 style보다는 특정 패턴, 혹은 색상을 넣는 것에 가까움 (처음 발견했을때 vgg만으로 content를 보존할 수 있다는 점은 굉장한 의미가 있는 방법이라고 생각됨)</p> <p><br></p> <h3 id="-model-approaches">👾 Model approaches</h3> <hr> <p><em>12.</em></p> <h4 id="how-does-uegan-preserves-content-details"><em>How does UEGAN preserves content details</em></h4> <p>UEGAN을 보면 conent는 preserving하면서 특히 colour 정보를 잘 변형하는 것을 볼 수 있다.</p> <p>👹 현재까지의 Limitation</p> <ol> <li>색감 변환이 과도하게 되는 경향이 있음 (L_qual의 수치를 조절해야 될 듯함 0.1이상에서는 too heavy하게 변형이 일어남)</li> <li>Grain에 해당 하는 내용은 synthesis하지 못함 (DL 특성상 2가지 task를 한번에 해결하려고 하면 더 복잡할 것으로 예상됨)</li> </ol> <p>🤞 UEGAN의 이점</p> <ol> <li>GAM(global attention Module)을 사용해서 illumination이나 colour를 더 잘 포착하고 집중하도록 설계됨</li> <li>3가지 loss(Quality, Fidelity, Identity)를 이용해서 각각 (enhancement, content preserve, over-enhancement preserve)</li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/uegan_test_pair-480.webp 480w,/assets/img/d2f/uegan_test_pair-800.webp 800w,/assets/img/d2f/uegan_test_pair-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/uegan_test_pair.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>이번 task의 핵심은 완벽할 정도의 qualitive score라고 생각함ㅇㅇ</p> <hr> <p><em>13.</em></p> <h4 id="rgb-grain-analysis"><em>RGB grain analysis</em></h4> <p>확실히 차이가 많이 남 (같은 왼쪽 상단 위치여서 하늘색 단색 부분임)</p> <p>👺 이게 제일 중요한 내가 봤을때는..</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/RGB_grain-480.webp 480w,/assets/img/d2f/RGB_grain-800.webp 800w,/assets/img/d2f/RGB_grain-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/RGB_grain.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <p><em>14.</em></p> <h5 id="spatial-analysis"><em>spatial analysis</em></h5> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/3d_film_R-480.webp 480w,/assets/img/d2f/3d_film_R-800.webp 800w,/assets/img/d2f/3d_film_R-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/3d_film_R.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>Film 3D R channel value</small></div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/3d_digi_R-480.webp 480w,/assets/img/d2f/3d_digi_R-800.webp 800w,/assets/img/d2f/3d_digi_R-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/3d_digi_R.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>Digital 3D R channel value</small></div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/3d_overlay-480.webp 480w,/assets/img/d2f/3d_overlay-800.webp 800w,/assets/img/d2f/3d_overlay-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/3d_overlay.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>Overlay of two</small></div> </div> </div> <hr> <p><em>15.</em></p> <h4 id="the-key-points11"><em>The key Points!11!!!!!!</em></h4> <p>3x3 stride=1로 max_val이 있는 곳을 255로 채운 결과물 -&gt; 즉 film의 silver halide가 존재함을 역으로 알 수 있다.</p> <p>그리고 그 surface 특성을 가지도록 하는것이 핵심일 것으로 보임 (무조건 RGB 채널 따로하는게 맞음)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/result_digital_mountain_R-480.webp 480w,/assets/img/d2f/result_digital_mountain_R-800.webp 800w,/assets/img/d2f/result_digital_mountain_R-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/result_digital_mountain_R.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/result_film_mountain_R-480.webp 480w,/assets/img/d2f/result_film_mountain_R-800.webp 800w,/assets/img/d2f/result_film_mountain_R-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/result_film_mountain_R.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <hr> <p><em>16.</em></p> <h4 id="max-value-interpolation"><em>Max value interpolation</em></h4> <p>interpolationd에서 min value에 대한 정보가 손실되서 grain에 대한 특성을 max_val pooling이 대표할 수 있을 것으로 생각되지만,</p> <p>혹여 grain한 pixel halide selection으로 GAN 학습 input으로 사용했을 때 해당 detail의 보존 및 재생 여부는 미지라고 생각함</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/bilateral_restored_film_image-480.webp 480w,/assets/img/d2f/bilateral_restored_film_image-800.webp 800w,/assets/img/d2f/bilateral_restored_film_image-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/bilateral_restored_film_image.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/combined_pooling-480.webp 480w,/assets/img/d2f/combined_pooling-800.webp 800w,/assets/img/d2f/combined_pooling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/combined_pooling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Combined pooling and interpolated results</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/combined_pooling_crop-480.webp 480w,/assets/img/d2f/combined_pooling_crop-800.webp 800w,/assets/img/d2f/combined_pooling_crop-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/combined_pooling_crop.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>Min pooling</small></div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/combined_minmax_pooling-480.webp 480w,/assets/img/d2f/combined_minmax_pooling-800.webp 800w,/assets/img/d2f/combined_minmax_pooling-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/combined_minmax_pooling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>Combined(Min/Max) pooling</small></div> </div> </div> <p>아래 방식이 좀 더 grain하게 분포함 -&gt; 이거를 input으로 활용할 예정</p> <p>RGB mask 각각의 density</p> <table> <thead> <tr> <th>Type</th> <th>Density of R / 1s</th> <th>Density of G / 1s</th> <th>Density of B / 1s</th> </tr> </thead> <tbody> <tr> <td>Film full size</td> <td>0.10262</td> <td>0.10287</td> <td>0.17325</td> </tr> <tr> <td>Film crop size</td> <td>0.10365</td> <td>0.095427</td> <td>0.1106</td> </tr> <tr> <td>Digital full</td> <td>0.43867</td> <td>0.4313</td> <td>0.44063</td> </tr> </tbody> </table> <p>일반화 가능하다고 여겨짐</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/random_noise_recon-480.webp 480w,/assets/img/d2f/random_noise_recon-800.webp 800w,/assets/img/d2f/random_noise_recon-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/random_noise_recon.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/mask_recons-480.webp 480w,/assets/img/d2f/mask_recons-800.webp 800w,/assets/img/d2f/mask_recons-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/mask_recons.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>💡 min pooling을 할때, 255반전에서 max pooling을 시켜서 진행하여서 masking이 정상적으로 가능하도록 함</p> <p>주의해야되는게, 아애 red가 0으로 되버리는 구간도 존재하니까 그런 구간에서의 pp halide를 어떻게 설정할지에 대한 고민이 필요해 보임</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/000006960025_mask-480.webp 480w,/assets/img/d2f/000006960025_mask-800.webp 800w,/assets/img/d2f/000006960025_mask-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/000006960025_mask.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> </div> </div> <p>python으로 loading한 data</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/pp_dataloaded-480.webp 480w,/assets/img/d2f/pp_dataloaded-800.webp 800w,/assets/img/d2f/pp_dataloaded-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/pp_dataloaded.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/pp_data_overexposued-480.webp 480w,/assets/img/d2f/pp_data_overexposued-800.webp 800w,/assets/img/d2f/pp_data_overexposued-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/pp_data_overexposued.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>250310 : pp halide에 대해서 1차 결론을 내림 / 3x3 window에서 마루와 골에 대한 mask를 칠하면 여전히 random성을 보장해 주는 것을 확인함</p> <p>pp halide에 대해서 max min pooling을 하고 해당 mask와 result png(무손실 이미지 파일)에 대해서 network를 구성하고 진행하면 reconsturction이 기대에 미치게 나올 것으로 예상</p> <p>digital image에 대해서는 밝기 기반 mask pp halide를 뿌리고 해당 pixel에서의 value를 기반으로 network로 reconstruction</p> <p>(why? digital image는 이미 밀어버린 image이기 때문에 min max를 해당 pixel로 설정하여도 무리가 없다고 판단)</p> <p>다만 색감에 있어서 해당 pp halide의 분포를 그냥 1d vector로 representative transformation을 해도 상관없을 것으로 예상됨</p> <p>왜냐하면 pixel value가 과연 렌즈와 film fixing에서 structural한 global한 정보를 반영하는지 모르겠음ㅇㅇ</p> <p>이때 아애 어두운 부분에 대해서 blue mask가 굉장히 밀도가 높은 모습들이 포착됨 (여기서 원래라면 film은 max value만이 의미를 가지지만 결국 모방을 굳이 똑같이 할 필요는 없다는 생각으로 desnity가 거의 0에 가까운 부분도 pp halide로 추출하기로 결정 -&gt; detail 및 edge 정보 유지)</p> <p>어두운 부분과 밝은 부분에 대해서 pp halide random seed를 density 밀도를 다르게 해서 digital 이미지에 대해서 적용할 것이기 떄문에 별로 문제가 된다고 생각하지는 않음</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/full_dataloaded-480.webp 480w,/assets/img/d2f/full_dataloaded-800.webp 800w,/assets/img/d2f/full_dataloaded-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/full_dataloaded.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <p><em>17.</em></p> <h4 id="dataloader-composition"><em>Dataloader composition</em></h4> <ol> <li>256x256 Crop</li> <li>randomhorizontal filp(0.5)</li> <li>manual_seed for cropping same regions for 3 image input(pp halide(result, mask), og img)</li> <li>croping 3 times random for same image for data augmentation</li> </ol> <p>🤔 hmm… Attention을 사용할 수 있을 것 같은데, 어떻게 보면 maskr가 attention역할이니까 🎀 uncertain한 것은 과연 모델이 detail을 살릴 수 있을지, 그리고 rgb 채널을 독립적으로 학습시켰을때, rgb간의 harmony를 유지할 수 있는지가 강권을 것으로 예상됨</p> <hr> <p><em>18.</em></p> <h4 id="unet-512-channel-based-ouptut"><em>UNET 512 channel based ouptut</em></h4> <table> <thead> <tr> <th>Trial</th> <th>Model</th> <th>Configuration</th> </tr> </thead> <tbody> <tr> <td>Trial1</td> <td>UNET128</td> <td>RGB + harmony</td> </tr> <tr> <td>Trial2</td> <td>UNET128</td> <td>harmony removed</td> </tr> <tr> <td>Trial3</td> <td>UNET128</td> <td>RGB + harmony</td> </tr> <tr> <td>Trial4</td> <td>UNET512</td> <td>crop size 256 / 4</td> </tr> <tr> <td>Trial5</td> <td>UNET512</td> <td>crop size 16 / 128</td> </tr> </tbody> </table> <ul> <li>epochs을 150정도를 학습시켜야 MSE loss가 0.0044 정도 나온다 (epoch 10으로는 학습X -&gt; trail 1,2,3를 다시 시도해볼 가치가 있음)</li> <li>trail4 방법에서 다소 mask image 값이 강조되는 경향이 있어서, 일반화 영향인가 의심되어 pixel value는 16x16 만드로도 충분히 interpolate 할 수 있다고 판단하고 trail 5 수행</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/trail5-480.webp 480w,/assets/img/d2f/trail5-800.webp 800w,/assets/img/d2f/trail5-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/trail5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>16x16이 조금 더 선명하고 salty하지 않은 이미지를 내보내긴하다.</p> <p>근데 여기서 salty는 png와 jpg의 차이라고 생각되기도 한다. (당장 홈페이지 렌더링만 봐도 smooth해졌음)</p> <p>📲 확실한 점은 디테일적인 부분을 캡쳐하지(다소 이미지의 RGB 채널간 뭉게진다고 생각됨) 못하다는 점에서 UNET이 아니라 오히려 <strong>channel방향보다 h,w 방향으로의 팽창</strong>을 생각해보고 싶어짐ㅇㅇ</p> <hr> <p><em>19.</em></p> <h4 id="digital-image-applicationstatistic-based---failed"><em>Digital Image Application(statistic based) - Failed</em></h4> <p>단순히 RGB channel 별 mask와 brightness의 관계를 파악해서 mask를 만들고 trail5 model로 reconstruction 해 볼 예정</p> <p>Analyzing dataset: 100%|██████████| 375/375 [01:11&lt;00:00, 5.25it/s]<br> Average Mask R pixel density:<br> Value 0: 0.8809 (±0.0329)<br> Value 255: 0.1191 (±0.0329)<br> <br> Average Mask G pixel density:<br> Value 0: 0.8902 (±0.0149)<br> Value 255: 0.1098 (±0.0149)<br> <br> Average Mask B pixel density:<br> Value 0: 0.8727 (±0.0418)<br> Value 255: 0.1273 (±0.0418)<br></p> <p>실망스럽죠ㅇㅇ</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/test_output_batch1_img1-480.webp 480w,/assets/img/d2f/test_output_batch1_img1-800.webp 800w,/assets/img/d2f/test_output_batch1_img1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/test_output_batch1_img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>거의 동일한 결과물을 가져옴 -&gt; 물론 이상한 artifact도 발생함ㅇㅇ (레인보우 artifact가 존재함ㅇㅇ)</p> <p>🥅 Goal 수정 -&gt; 일단 maks와 result (pp halide)로 reconstruction이 가능하다는 것을 알았기 때문에, digital 사진에 어떻게 pp halide를 분포 시킬지에 대한 접근으로 생각하자</p> <p>태초에 pp halide로 접근했던 목적이 -&gt; grain synthesis를 더 쉽게 하고자 했던 것이기 때문에, 일단은 digital image에 대해서 grain synthesis가 가능한가에 대한 결과를 보고나서, 색감(colour)로 넘어갈 것임</p> <p>따라서 일단 detail을 위해서만을 위해서 추가했던, min pooling을 빼고 max pooling만으로 구성된 datset르 가지고 UNET이 과연 reconstruction을 옳바르게 할 수 있는지를 확인할 것</p> <hr> <p><em>20.</em></p> <h4 id="max-poolingonly-dataset-training---for-the-grain"><em>Max pooling(only) dataset training - for the grain</em></h4> <p>와 일단 지금까지 가장 효과적인 방법 발견!</p> <ol> <li>DIANET을 정의함 (위에서 말했던 UNET과 반대의 구조로 channel과 hw가 팽창했다가 줄어드는 구조임 -&gt; skip connection도 존재)</li> <li>이때 channel의 수가 너무 커지면 model size가 굉장히 커짐</li> <li>32x32 patch x 64로 학습 진행함</li> <li> <em>Digital image에서 pp halide 밀도를 2배로 올림ㅇㅇㅇ</em> (본래 min max density인 0.1에서 2배, 3배로 올리니까 artifact가 확실히 줄어들면서 grainy한 이미지가 생성됨)</li> <li>(limitation1) size(3024x3024)가 커졌을대 UNET에선 발생하지 않았던 Memory 부족이 발생함</li> <li>(limitation2) colour error가 심함 ➡️ film L2 loss에서 colour에 대한 내용을 집중하지 못하고 structural한 내용에 집중하다가 학습이 끝난것 같은 느낌</li> </ol> <ul> <li>Densiety * 3 (approx 0.3)</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/dianet_digital_denx3-480.webp 480w,/assets/img/d2f/dianet_digital_denx3-800.webp 800w,/assets/img/d2f/dianet_digital_denx3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/dianet_digital_denx3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Density * 1</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/dianet_digital_denx1-480.webp 480w,/assets/img/d2f/dianet_digital_denx1-800.webp 800w,/assets/img/d2f/dianet_digital_denx1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/dianet_digital_denx1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>물론 이때 density 뿐만 아니라 RGB pp halide간의 correlation도 통계를 내려서 분포 시킴</p> <p>💭 알 수 있는 점</p> <ul> <li>UNET과 DIANET은 확실히 생각한것과 같이 fine한 부분을 생성하는데 차이를 보인다.</li> <li>enc / dec 의 학습 output을 확인하면 초반에는 structural한 영역을 잡으려고 노력하고, 왠만큰 structural한 내용이 saturation되면 그때부터 색감 정보를 파악하려고 노력함</li> <li>UNET(0.0044), DIANET(0.017) 의 L2 loss가 차이가 많이나는 모습을 보이는데, DIANET은 structural 정보를 많이 살리지 못함(why? min pooling을 삭제했기 때문에 edge나 black spot에 대한 inference에서 약한것으로 예상됨)</li> <li>parmeter 수는 DIANET이 더 적을지 몰라도 image를 transposedConv2d로 upscaling하면서 추가적으로 channel도 키워서 memory 사용량이 급증함</li> <li>반면 UNET의 경우 channel을 늘리지만 hw도 줄어들기 때문에 memory expolde가 발생하지는 않음</li> </ul> <p><em>only max</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/UNET_max_infer-480.webp 480w,/assets/img/d2f/UNET_max_infer-800.webp 800w,/assets/img/d2f/UNET_max_infer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/UNET_max_infer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><em>max min pooling</em></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/UNET_maxmin_infer-480.webp 480w,/assets/img/d2f/UNET_maxmin_infer-800.webp 800w,/assets/img/d2f/UNET_maxmin_infer-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/UNET_maxmin_infer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Digital applied Comparison</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/UNET_512_minmax-480.webp 480w,/assets/img/d2f/UNET_512_minmax-800.webp 800w,/assets/img/d2f/UNET_512_minmax-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/UNET_512_minmax.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>UNET (Min-Max)</small></div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/UNET512_max-480.webp 480w,/assets/img/d2f/UNET512_max-800.webp 800w,/assets/img/d2f/UNET512_max-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/UNET512_max.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>UNET (Max)</small></div> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/DIANET_max-480.webp 480w,/assets/img/d2f/DIANET_max-800.webp 800w,/assets/img/d2f/DIANET_max-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/DIANET_max.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div align="center"><small>DIANET (Max) + density * 2</small></div> </div> </div> <p>가장 deep 한 layer에서 UNET512와 DIANET(3enc)의 float32에서의 메모리 usage는</p> <table> <thead> <tr> <th>모델</th> <th>출력 텐서 크기</th> <th>총 요소 수</th> <th>메모리 사용량</th> <th>비고</th> </tr> </thead> <tbody> <tr> <td>UNET</td> <td>512x16x16</td> <td>131,072</td> <td>0.5 MB (524,288 B)</td> <td>-</td> </tr> <tr> <td>DIANET</td> <td>24x1024x1024</td> <td>25,165,824</td> <td>96 MB (100,663,296 B)</td> <td>192x</td> </tr> </tbody> </table> <p><br> 💭 알 수 있는 점</p> <ul> <li>model parameter size는 UNET이 훨씬 많지만, memory usage는 DIANET이 많음 (UNET이 가진 강점을 볼 수 있음)</li> <li>trade-off (memory vs parameter size)</li> </ul> <p><br></p> <h3 id="️-loss-functions">🏞️ Loss functions</h3> <hr> <p><em>21.</em></p> <h4 id="importance-of-loss-function"><em>Importance of loss function</em></h4> <p>LAB으로 하니까 contrast가 너무 쎄지는 현상 발생</p> <p>dynamic loss function 사용</p> <p>오늘은 기존의 단순한 RGB 기반 MSE loss만 사용하는 방식에서 벗어나서, RGB MSE, LPIPS, SSIM, Gram Matrix(texture)를 조합한 dynamic loss 방식을 도입했어. 초기엔 structural 정보(SSIM)와 perceptual 정보(LPIPS)에 집중하고, 후반엔 color(RGB)와 세부적인 grain 표현(Gram Loss)에 집중하도록 가중치를 조정하는 전략을 선택했어. 추가로 각 loss 간의 크기(scale) 불균형 문제를 해결하기 위해 normalization 또는 adaptive weighting 전략을 고민했어. TensorBoard를 이용해 loss를 실시간으로 추적하고 시각화하도록 구성했어. 🌟</p> <ul> <li>MSE 기반은 structural 정보를 capture하기는 하지만 강하지 않음</li> <li>local texture(grain) 정보 (fine details) 를 표현할 수 없음 (왜냐하면 loss를 줄이기 위해서 fine한 random성을 예측할바에는 그냥 mean으로 texture 죽이고 평탄화해서 경향성만 맞추려고 함)</li> </ul> <p>오히려 이렇게 loss function을 설정하고 나니가 dianet이 detail 표현 capa에 무리가 있다고 생각됨 (너무 32x32에 fit해 져서 2048x2048에서도 다소 두탁하고 넓은 artifact로 나오는듯하다 150epoch까지 일단 보고 결정)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/dianet_artifact_dynamic_loss-480.webp 480w,/assets/img/d2f/dianet_artifact_dynamic_loss-800.webp 800w,/assets/img/d2f/dianet_artifact_dynamic_loss-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/dianet_artifact_dynamic_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>따라서 loss를 dynamic하게 설정하고 UNET이 과연 fine detatail을 살릴 수 있을지 지켜보자.</p> <p><br></p> <h5 id="-총-손실함수는-다음과-같이-구성됨">🥊 총 손실함수는 다음과 같이 구성됨:</h5> <p>Total Loss = α × (RGB MSE) + β × (LPIPS) + γ × (SSIM) + δ × (Gram Loss)</p> <h5 id="-단계별-scaling-factor-가중치-조정">📌 단계별 Scaling Factor (가중치 조정)</h5> <table> <thead> <tr> <th>진행률 (Epoch %)</th> <th>RGB MSE (α)</th> <th>LPIPS (β)</th> <th>SSIM (γ)</th> <th>Gram Loss (δ)</th> <th>집중 요소</th> </tr> </thead> <tbody> <tr> <td><strong>0 ~ 30%</strong></td> <td>0.2</td> <td>0.7</td> <td>0.8</td> <td>0.3</td> <td>Structure (SSIM), Perceptual (LPIPS)</td> </tr> <tr> <td><strong>30 ~ 60%</strong></td> <td>0.5</td> <td>0.4</td> <td>0.4</td> <td>0.5</td> <td>균형 잡힌 학습 (Balanced)</td> </tr> <tr> <td><strong>60 ~ 100%</strong></td> <td>0.8</td> <td>0.3</td> <td>0.2</td> <td>0.6</td> <td>Color 미세조정 (RGB), 질감 (Gram)</td> </tr> </tbody> </table> <ul> <li> <strong>초기(0~30%)</strong>: 구조적 및 perceptual 정보 학습 중심</li> <li> <strong>중기(30~60%)</strong>: 균형 잡힌 학습 진행</li> <li> <strong>후기(60~100%)</strong>: 세부 컬러와 grain 등 디테일한 부분 최적화</li> </ul> <p>tensorboard로 loss 시각화를 시작하고, 동일 이미지에 대해서 gif르 구성하고 학습과정을 보는데, 일단 perceptual loss가 들어가면 색상정보를 거의 학습하지 않음</p> <figure> <picture> <img src="/assets/img/d2f/training_progress_interrupted.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>그리고 loss가 학습과정 중에 감소한다고 보기 어렵고 횡보한다고 생각함.</p> <p>여기서 중요한 점은 결국 이미지의 색상정보나 structual 한 정보를 단순히 pixel value별로 비교하는것은 의미가 없어보임</p> <p>➡️ 왜냐하면 film image 자체가 random성을 많이 가지고 있다고 생각되는데, 이를 pixel by pixel loss로 접근하거나 structual한 loss로 접근하면 학습이 용이하지 않다고 생각됨</p> <p>➡️ 또한 model의 capa에도 학계가 있다고 느껴지며, 동일한 패턴만을 학습하는 것은 의미가 없다고 생각됨 (위와 동일한 맥락).</p> <p>따라서 random feature를 eject하는 부분이 있으면 좋을 것 같다는 생각임</p> <p>https://medium.com/storm-shelter/the-importance-of-film-grain-255f0246cd64</p> <p>동영상에서 grain synthesis랑 film photography에서의 grain synthesis랑 햇갈릴 수 있음</p> <p>상업 카메라에서 Digital Image pipeline(ISP)는 상당히 중요한 의미를 가지는 것으로 생각됨</p> <p>RAW에서부터 JPG로의 1차 가공이기에 그 회사가 가지고 있는 기술력과 색감 특성을 보여줘야하는 가장 중요한 단계라고 생각됨</p> <p>🧐 어, 엄청난 접근법이 생각남 -&gt; 아애 no base로 synthesis하는게 아니라 내가 주는 dataset을 바탕으로 patch단위 혹은 ststistic을 가지고서 마치 색종이 붙이처럼 digital image에 맞는 patch를 붙이는 방법도 괜찮을 것 같음 -&gt; 이러면 transformer를 사용해 볼 수도 있을듯 (내가 가지는 이미지의 kq로 이미지 가지고 있는 dataset와의 value를 계산해보면?)</p> <p>[SCW06] STEFANO A. D., COLLIS W., WHITE P. R.: Synthesising and reducing film grain. Journal of Visual Communication and Image Representation 17, 1 (2006), 163–82.</p> <p>여기서 그렇게 했다는데 나중에 읽어볼 것!</p> <p><br></p> <h3 id="-other-approaches">💨 Other Approaches</h3> <hr> <p><em>22.</em></p> <h4 id="cvae-method-pre-test"><em>CVAE Method pre-test</em></h4> <p>🌈 dataloader에서 하나의 이미지에 대해서 여러개의 patch를 굳이 설정하지 않아도, dataset의 <code class="language-plaintext highlighter-rouge">__len__</code> property를 조절해서 하나의 이미지에서 여러개의 patch가 나오도록 조절할 수 있음</p> <p>CVAE 탈락</p> <div style="border: 1px solid var(--global-theme-color-light); padding: 1em; border-radius: 4px; background-color: var(--global-theme-bg);"> <p>교훈</p> <ul> <li>8x8 patch로 batch size 64로 학습했는데 gpu 사용을 효율적으로 하지 못하는 것을 발견함</li> <li>즉 8x8을 위해서 오히려 더 많은 Image RW operation이 발생해서 상대적으로 bottleneck 이 그부분에 걸림</li> <li>그래서 이미지를 메모리에 올려놓고, 8x8 patch를 extracting하는 방식을 사용</li> </ul> </div> <p><br></p> <p><em>22.</em></p> <h4 id="some-technics-prefetch_generator-pin_memory"><em>Some technics (prefetch_generator, pin_memory)</em></h4> <h5 id="prefetch_generatorbackgroundgenerator">prefetch_generator(BackgroundGenerator)</h5> <div style="border: 1px solid var(--global-theme-color-light); padding: 1em; border-radius: 4px; background-color: var(--global-theme-bg);"> <p><em>For 375 image data (3089 × 2048)</em></p> <ul> <li>wo BackgroundGenerator : 45.65초</li> <li>w BackgroundGenerator : 26.78초</li> </ul> </div> <p><br> 📋 이때, 학습 delay를 0.1ms로 주었을때 효과가 들어나지 / 그냥 dataloader만 순환시키면 효과가 들어나지 않음<br> 따라서 학습 시 num_woker를 사용할 수 없는 apple silicon device에 사용하면 효과가 있을 것으로 생각됨</p> <h5 id="pin_memory">pin_memory</h5> <p>Memory는 운영체제에서 사용하지 않을 시 디스크 공간으로 swap하도록 관리가능한 pageable memory와 운영체제에서 swap 못하는 page-locked memory가 존재함<br> 이때, pin_memory 설정을 false하게 되면 pageable memory에 data가 불러와지게 되고, gpu device mem공간으로 옴기기 위해서는 page-loacked memory로 복사 + DMA를 사용하여 GPU VRAM으로 전송 2단계를 거쳐야함. 여기서 overhead 발생<br> 하지만 pin_memory를 사용하게 되면 page-locked memory로 data가 불러와지기 때문에 overhead가 줄어듬</p> <p>1000개의 random tensor summation에 대해서 pin_memory 유무 속도 비교</p> <div style="border: 1px solid var(--global-theme-color-light); padding: 1em; border-radius: 4px; background-color: var(--global-theme-bg);"> <p><strong>사용 중인 디바이스: mps</strong></p> <ul> <li>📈 증가한 Page-Locked Memory: 1078.12 MB</li> <li>🕒 pin_memory=False 소요 시간: 0.80초</li> </ul> <p><strong>사용 중인 디바이스: mps</strong></p> <ul> <li>📈 증가한 Page-Locked Memory: 1080.55 MB</li> <li>🕒 pin_memory=True 소요 시간: 0.81초</li> </ul> </div> <p>Unified memory를 사용하는 M2 macbook에 대해서는 적용이 안된다고 생각됨, test결과 그냥</p> <p>pytorch의 소스코드를 보면<br> 1042번줄에서 mps device에서의 pin_memory option은 관리를 하지 않음. mps 자체에서 메모리를 관리를 하는듯 보인다. 이게 UM의 특성이라고 생각됨.</p> <p><a href="https://github.com/pytorch/pytorch/blob/e53d9590287cbf97521f96d055910394f6e9a849/torch/utils/data/dataloader.py#L1042-L1064" rel="external nofollow noopener" target="_blank">pytorch dataloader.py source code</a></p> <p><a href="https://github.com/pytorch/pytorch/blob/main/aten/src/ATen/mps/MPSAllocator.mm" rel="external nofollow noopener" target="_blank">pytorch mps MPSAllocator.mm source code</a></p> <p>.mm file은 objective-C와 C++을 섞은 형태 ➡️ pytorch(c++ 기반), Metal API는 Object-C 기반 ➡️ 2개 연결하려면 .mm file로 연결</p> <h5 id="-mach-virtual-memory-statistics-page-size-16384-bytes">📊 Mach Virtual Memory Statistics (Page size: 16,384 bytes)</h5> <p><em>vm_stat</em> command</p> <h5 id="available-memory">*Available Memory**</h5> <ul> <li> <strong>Pages free:</strong> 18,064 (<strong>≈ 281MB</strong>)</li> </ul> <h5 id="memory-usage">*Memory Usage**</h5> <ul> <li> <strong>Pages active:</strong> 302,460 (<strong>≈ 4.7GB</strong>)</li> <li> <strong>Pages inactive:</strong> 300,448 (<strong>≈ 4.7GB</strong>)</li> <li> <strong>Pages speculative:</strong> 933 (<strong>≈ 15MB</strong>)</li> </ul> <h5 id="page-locked-memory-pinned-memory">*Page-Locked Memory (Pinned Memory)**</h5> <ul> <li> <strong>Pages wired down:</strong> 113,599 (<strong>≈ 1.8GB</strong>)</li> </ul> <h5 id="cached--purgeable-memory">*Cached &amp; Purgeable Memory**</h5> <ul> <li> <strong>Pages purgeable:</strong> 7,490 (<strong>≈ 120MB</strong>)</li> <li> <strong>File-backed pages:</strong> 139,514 (<strong>≈ 2.2GB</strong>)</li> <li> <strong>Anonymous pages:</strong> 464,327 (<strong>≈ 7.4GB</strong>)</li> </ul> <h5 id="memory-compression">*Memory Compression**</h5> <ul> <li> <strong>Pages stored in compressor:</strong> 722,020 (<strong>≈ 11.5GB</strong>)</li> <li> <strong>Pages occupied by compressor:</strong> 275,678 (<strong>≈ 4.4GB</strong>)</li> <li> <strong>Decompressions:</strong> 2,864,785</li> <li> <strong>Compressions:</strong> 5,081,194</li> </ul> <h5 id="swap--paging">*Swap &amp; Paging**</h5> <ul> <li> <strong>Pageins:</strong> 2,560,652</li> <li> <strong>Pageouts:</strong> 18,275</li> <li> <strong>Swapins:</strong> 122,093</li> <li> <strong>Swapouts:</strong> 661,024</li> </ul> <h5 id="additional-info">*Additional Info**</h5> <ul> <li> <strong>Translation faults:</strong> 74,863,824</li> <li> <strong>Pages copy-on-write:</strong> 5,963,520</li> <li> <strong>Pages zero filled:</strong> 29,007,477</li> <li> <strong>Pages reactivated:</strong> 2,205,490</li> <li> <strong>Pages purged:</strong> 634,520</li> </ul> <hr> <p><em>23.</em></p> <h4 id="model-architecture-visualization"><em>Model architecture visualization</em></h4> <p>netron.start(“unet.onnx”)</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/unet.onnx-480.webp 480w,/assets/img/d2f/unet.onnx-800.webp 800w,/assets/img/d2f/unet.onnx-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/unet.onnx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>DIANET visulization</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/d2f/dianet.onnx-480.webp 480w,/assets/img/d2f/dianet.onnx-800.webp 800w,/assets/img/d2f/dianet.onnx-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/d2f/dianet.onnx.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>ONNX (Open Neural Network Exchange)</strong> : 다양한 framework로 학습된 모델의 IR 역할을 하여서 HW회사들은 ONNX를 타게팅으로 compiler나 sw 스택을 최적화하면 됨</p> <p>NVIDIA : ONNX -&gt; TensorRT -&gt; CUDA kernel<br> Intel : ONNX -&gt; OpenVINO -&gt; ?<br> Qualcomm : ONNX -&gt; SNPE -&gt; HexagonDSP<br> APPLE : ONNX -&gt; CoreML -&gt; ANE(apple neural engine)<br></p> <hr> <p><em>24.</em></p> <h4 id="model-architecture-seeks-clean-code-done"><em>Model architecture seeks (clean code done!)</em></h4> <p>Vgg perceptual loss는 버리자ㅇㅇ -&gt; 색 재현을 못할 뿐더러 MSE loss만 있던게 더 나음</p> <p>Tries</p> <ol> <li>relu -&gt; leakyrelu</li> <li>concat 직전에 conv한번</li> <li>sigmoid 불가 (image input이 [-1, 1] 정규분포임) -&gt; 큰 문제였음</li> <li> </ol> <p>DiaNet도 구조가 문제인지 학습방법이 문제인지는 모르겠는데, 잘 안되는건 확실함</p> <hr> <p><em>25.</em></p> <h4 id="homogeniety-block-detection"><em>Homogeniety block detection</em></h4> <p>실험적으로 설정한 parameter : def extract_pure_color_patches(image_path, patch_size=16, stride=8, variance_threshold=300, edge_threshold=0.01):<br> homogeniety block detection을 위한 parameter</p> <p>detection 이미당 5만장 다 저장하다가 맥북 용량 다 잡아먹고, 삭제하는데도 파일 읽는데 엄청난 시간이 걸리는 문제 발생. ssd 속도가 확실히 느리다는것을 체감하는중 (적당한 사이즈로 dataset만들거나 dataloader에서 작동하도록 해야될듯)</p> <p>내가하는 taskr가 지금 low-level vision task(denoising, enhancement, SR) / high-level vision task (semantic tasks)</p> <p>Style trasnfer는 둘 다 고려해야되는 mid-level vision task로 불릴 수 있다고 한다.</p> <p>📃 Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training</p> <p>여기서 noise 자체의 randomness와 irregularity를 고려해서 이거를 L1 loss로 하는건 부적절 (non convergence)<br> 따라서 noise를 Random Variable로 여기고 MLE(Irn분포를 쫒악도록)와 Dd를 이용한 clean image alignment로 Image의 생성에서의 noise를 모방하도록 설정함<br> 본 논문에서는 Realistic Discriminator를 사용한다. 지금까지 가장 성능과 안정성이 좋았던 UEGAN은 Realistic Discriminator에서 한발짝 더 나아간 Relativistic average HingeGAN (RaHingeGAN)을 활용한다.</p> <p>PNGAN에서도 해당 기법을 적용해 볼 수 있겠다.</p> <hr> <p><em>26.</em></p> <h4 id="pngan-denoisier--uegan-or-others"><em>PNGAN denoisier + UEGAN (or others)</em></h4> <p>UEGAN이 일단 grain이 있을때도 성능을 보여주긴 했는데, denoising이 된 경우에 어떻게 학습이 진행되는지 비교를 위해서 해보자.</p> <p>Colour과 grain을 동시에 synthesis할 수 있는 방법은 따로 없어보이는데..</p> <p>https://developer.apple.com/videos/play/wwdc2024/10160/</p> <p>Torch 생태계 안에서 model을 fine-tunning하고 deployment하는 방법</p> <p>https://www.youtube.com/watch?v=SN-BISKo2lE</p> <hr> <p><em>27.</em></p> <h4 id="tpu-hbm-max-allocatoin"><em>TPU hbm Max allocatoin</em></h4> <p>PNGAN으로 film 이미지를 모두 denoising하려고 했는데, 또 메모리 부족 상태가 발생함 (그래서 colab tpu로 했는데도 32gb에서도 메모리 부족함)</p> <p>RuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: XLA:TPU compile permanent error. Ran out of memory in memory space hbm. Used 31.40G of 7.48G hbm. Exceeded hbm capacity by 23.92G.</p> <p>Total hbm usage &gt;= 31.92G: reserved 530.00M program 31.40G arguments 0B</p> <p>다시 생각해보니까, 애초에 UEGAN에서는 resize를 해서 noise 성분이 거의 영향을 안미쳤을 것이라고 생각된긴함</p> <p>따라서 denoising을 활용할 가능성이 낮을 것으로 예상</p> <hr> <p><em>28.</em></p> <h4 id="efficientdet"><em>EfficientDet</em></h4> <p>RCTNet + UEGAN + PNGAN 구현하려고 RCTNet 다시 읽다가 EfficientDet의 feature fusion을 사용했다고 해서 읽음</p> <p>JAX가 요즘 뜨는듯ㅇㅇ</p> <p>EfficientDet은 기존의 feature을 더 효율적으로 연산량을 효율적으로 사용하면 더 유의미한 feature를 뽑아내기 위한 시도라고 볼 수 있음</p> <p>지금 흐름이</p> <ol> <li>RCTNet</li> <li>UEGAN</li> <li>Stochastic film grain synthesis</li> <li>PNGAN</li> </ol> <p>을 바탕으로 해당 알고리즘, 모델, 학습방법을 fusion 하기 위해서 후속 논문이나 선행 연구 논문 읽기중</p> <ol> <li> <p>EfficientDet- Scalable and Efficient Object Detection (cited by RCTNet)</p> </li> <li> <p>MAXIM: Multi-Axis MLP for Image Processing (citing UEGAN) 나도 어떻게 보면 low-level vision tasks에서 multi-stage networks를 사용해야 될것으로 예상됨 이런 순간이 옴ㅇㅇ 읽다보면 너무 새로운 개념이라서 한번의 글자를 읽는 것만으로는 이해가 안되는 순간이 옴 -&gt; 굉장히 머리 아프고 자괴감이 들지만, 최대한 머리를 정리하고 다시 이해하려고 노력해서 한 step 진보한다는 마음으로 나아감</p> </li> <li> </ol> <p>heuristic-based scaling approach : 직관이나 경험적 판단, 간단한 규칙을 이용하는 접근</p> <p>Image enhancement 같은 경우에는 paper with code에서도 SOTA를 선정하는 기준이 대게 qualitivie comparision이기 때문에, 다소 주관적인 영역임</p> <p>💭💭💭 따라서 엔지니어의 artistic sense가 필요한 영역이라고 생각되고, 내가 그래서 흥미가 있다고 생각하는 듯하다. (SSIM, PSNR 등과 같은 수치적인 improvement는 나에게 다소 흥미롭게 안느껴짐)</p> <p>CUDA vs MPS</p> <p>MPS(M2 16gb) Average Process Time: 0.4536 sec Average Prepare Time: 0.0005 sec Average Compute Efficiency: 1.00</p> <p>CUDA(t4 colab) Average Process Time: 0.0309 sec Average Prepare Time: 0.0004 sec Average Compute Efficiency: 0.98</p> <p>⊳ Different Norms</p> <p>LayerNorm: Normalizes each sample over the last dimension (C), yielding B×H×W averages = 4×16×16 = 1024. 😊</p> <p>BatchNorm: Normalizes each channel over the entire batch (B, H, W), resulting in C averages = 64. 🚀</p> <p>InstanceNorm: Normalizes each channel per sample over spatial dimensions (H, W), giving B×C averages = 4×64 = 256. 👍</p> <p>These results match the PyTorch documentation and deep learning literature (Ba et al., Ioffe &amp; Szegedy).</p> <hr> <h3 id="-references">📌 References</h3> <p>🧷 https://ml-explore.github.io/mlx-data/build/html/index.html<br> 🧷 https://ml-explore.github.io/mlx/build/html/index.html<br> 🧷 https://blog.jaeyoon.io/2017/12/jekyll-image.html<br> 🧷 https://gyumpic.tistory.com/511<br></p> <hr> <h3 id="-papers">📃 Papers<br> </h3> <p><a href="https://openreview.net/forum?id=tIjRAiFmU3y" rel="external nofollow noopener" target="_blank">An Unsupervised Deep Learning Approach for Real-World Image Denoising</a><br> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="external nofollow noopener" target="_blank">Image Style Transfer Using Convolutional Neural Networks</a><br> <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.pdf" rel="external nofollow noopener" target="_blank">MAXIM: Multi-Axis MLP for Image Processing</a><br> <a href="https://arxiv.org/abs/1911.09070" rel="external nofollow noopener" target="_blank">EfficientDet: Scalable and Efficient Object Detection</a><br> <a href="https://ieeexplore.ieee.org/document/6698054" rel="external nofollow noopener" target="_blank">Investigating properties of film grain noise for film grain management</a><br> <a href="https://arxiv.org/abs/2204.02844" rel="external nofollow noopener" target="_blank">Learning to Generate Realistic Noisy Images via Pixel-level Noise-aware Adversarial Training</a><br> <a href="https://ieeexplore.ieee.org/document/4106554" rel="external nofollow noopener" target="_blank">Rapid and Reliable Detection of Film Grain Noise</a><br> <a href="https://eprints.bournemouth.ac.uk/10547/1/grain.pdf" rel="external nofollow noopener" target="_blank">Simulating Film Grain using the Noise-Power Spectrum</a><br> <a href="https://arxiv.org/abs/1505.07376" rel="external nofollow noopener" target="_blank">Texture Synthesis Using Convolutional Neural Networks</a><br> <a href="https://link.springer.com/article/10.1007/s00521-023-09283-5" rel="external nofollow noopener" target="_blank">Film-GAN: towards realistic analog film photo generation</a><br> <a href="https://arxiv.org/abs/2307.03992" rel="external nofollow noopener" target="_blank">Stimulating Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling</a><br> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12146" rel="external nofollow noopener" target="_blank">Computational Simulation of Alternative Photographic Processes</a><br> <a href="https://hal.science/hal-01520260/file/Film_grain_synthesis_computer_graphics_forum.pdf" rel="external nofollow noopener" target="_blank">A Stochastic Film Grain Model for Resolution-Independent Rendering</a><br> <a href="https://www.ijcai.org/proceedings/2023/0129.pdf" rel="external nofollow noopener" target="_blank">A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement</a><br> <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Deep_Photo_Enhancer_CVPR_2018_paper.pdf" rel="external nofollow noopener" target="_blank">Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs</a><br> <a href="https://arxiv.org/abs/2007.15651" rel="external nofollow noopener" target="_blank">Contrastive Learning for Unpaired Image-to-Image Translation</a><br> <a href="https://arxiv.org/abs/2206.07411" rel="external nofollow noopener" target="_blank">Deep-based Film Grain Removal and Synthesis</a><br> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700341.pdf" rel="external nofollow noopener" target="_blank">Global and Local Enhancement Networks for Paired and Unpaired Image Enhancement</a><br> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Kim_Representative_Color_Transform_for_Image_Enhancement_ICCV_2021_paper.pdf" rel="external nofollow noopener" target="_blank">Representative Color Transform for Image Enhancement</a><br> <a href="https://hywang99.github.io/2022/07/09/lcdpnet/" rel="external nofollow noopener" target="_blank">Local Color Distributions Prior for Image Enhancement</a><br> <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123750375.pdf" rel="external nofollow noopener" target="_blank">PieNet: Personalized Image Enhancement Network</a><br> <a href="https://arxiv.org/abs/2012.15020" rel="external nofollow noopener" target="_blank">Towards Unsupervised Deep Image Enhancement with Generative Adversarial Network</a><br> <a href="https://arxiv.org/abs/1505.04597" rel="external nofollow noopener" target="_blank">U-Net: Convolutional Networks for Biomedical Image Segmentation</a><br> <a href="https://arxiv.org/abs/1703.10593" rel="external nofollow noopener" target="_blank">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a><br></p> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Duhyeon Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. All photos from <a href="https://www.linkedin.com/feed/" target="_blank" rel="external nofollow noopener">Duhyeon Kim</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-PL3J05TNB5"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-PL3J05TNB5');
  </script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>