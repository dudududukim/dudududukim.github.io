---
title: "OpenVLA: An Open-Source Vision-Language-Action Model"
date: 2025-12-26
section: "tech-bites"
categories: ["AI", "Paper Review"]
description: "review of https://arxiv.org/pdf/2406.09246"
reading_time: 3
published: false
---

> Original Paper : [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/pdf/2406.09246)
<br>OpenSource repo : [OpenVLA](https://openvla.github.io/)

## Abstract



## Model architecture

기본적인 VLM 기반으로, vision encoder 2개를 concat해서 Prismatic-7B VLM에 넣으면 action token이 나오는 형식.

기존의 모델들과 차이점으로 강조하는 3가지

1. Performance in general multiple tasks
2. Full open-source
3. Fine-tunning framework supports


best practice BF16 precision in inference.

non-Blocking control과 Blocking Control이 꽤 유의미한 future study 방향성을 제시한다고 생각함.

"Making non-Blocking control possible"