---
title: "VLA-0: Building State-of-the-Art VLAs with Zero Modification"
date: 2026-01-02
section: "tech-bites"
categories: ["AI", "Paper Review"]
tags: ["VLA", "VLA-0", "Text-based action control"]
description: "review of https://arxiv.org/pdf/2510.13054"
reading_time: 3
published: true
---

> Original Paper : [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://arxiv.org/pdf/2510.13054)
> <br>OpenSource repo : [https://vla0.github.io/](https://vla0.github.io/)
{: .reference}

## Introductions

#### 1. Discrete Token VLA (RT-2 / Open-VLA)

> Key architecture : Robot actions, originally continuous, are discretized into bins; each bin is then assigned a token from the VLM vocabulary, using either new or infrequent tokens

Mostly made by google, it has two limitations.

- As it restrict the actions into discrete bins, fine-grained control is limited. (resolution of actions)
- It compromises pretrained language understanding of the VLM by repurposing its vocabulary for actions

#### 2. Generative Action Head VLAs

> Key architecture : The VLM is fine-tuned to predict a latent vector, which is then decoded into actions using a generative model such as a diffusion process or flow matching.

The primary drawback is,<br>This often leads to a __decline in the language understanding__ and grounding capabilities of the underlying VLM.

#### 3. Custom Architecture VLAs

> It contains architectural modifications or custom tokenizers tailored to action prediction.

But, significant architectural changes, additional parameters, or custom training pipelines.

<br>

## 2. Related Works

The key to our success lies in a carefully designed training and inference recipe, including action token masking and prediction ensembling, a critical component not explored in LLARVA(2-stage text based action prediction).

<br>

## 3. Methods

### VLM (Vision-Languge-Mdoel)

1. Pre-trained Encoder -> Projecting patches of Image into LLM's embedding space.
2. LLM tokenizer -> Embedding input text.

Backbone : __`Qwen-VL-2.5-3B model`__
<br>For reproducibility, compute efficiency, competitive performance for its model size.

### VLA-0

> VLA-0 preserves the integrity of the underlying VLM : it does not introduce new tokens, alter the existing vocabulary, or add any new neural network layers.

However, achieving this performance relies on a __*careful recipe*__.

#### Inputs

*System Prompt*, *Images*, and a *Task Instruction*.

- System Prompts : 

```
Analyze the input image and predict robot actions for the next H timesteps. Each action has D dimensions. Output a single sequence of H× Dintegers (0 - B each), representing the H timesteps sequentially. Provide only space-separated numbers. Nothing else.
```

- Images : 

<br> Simulation : (Third person view + wrist view) like basemodel
<br> Real test : right / left image

- Task Instruction :

e.g. *“put the banana on the plate."*

#### Action Decoding

As VLA-0 returns the text generated action.

To simplify this task, we ask the VLM to output actions as integers.

#### Ensemble Prediction

At each inference step, the VLM predicts a sequence of nfuture actions.

We average these npredictions to produce the final, more stable action at time step t.

<img src="/assets/images/posts/method-ACT.png" alt="average-n-step-ACT" width="400">

#### Masked Action Augmentation (training augmentation)

> During training, we randomly mask out characters in the target action string. <br>This procedure forces the VLM to reason about the action based on the visual observation and instruction, rather than simply relying on auto-completing a numerical sequence it has started to generate.

<br>

## 4. EXPERIMENTS

### Setup

#3## Tasks (World)

- reorienting ablock
- pushing an apple
- picking and placing a banana
- picking and placing a cupcake.

For each task, we collect 100 demonstrations for training.

#### Tasks (Simulation)

LIBERO consists of four suites: Spatial, Object, Goal, and Long.

Each suite contains 10 tasks, and each task is tested over 50 episodes.

### Baselines

See the Below table.

### Tables
<table style="border-collapse:collapse; width:100%; font-size:12px; font-family:sans-serif; text-align:center;">
  <thead>
    <tr style="border-top:2px solid black; border-bottom:1px solid black; background:#f9f9f9;">
      <th style="text-align:left; padding:4px;">Models</th>
      <th>Large-scale<br>pre-train</th>
      <th>VLA Type</th>
      <th>Spatial</th>
      <th>Object</th>
      <th>Goal</th>
      <th>Long</th>
      <th>Avg.</th>
      <th>Avg. rank</th>
    </tr>
  </thead>
  <tbody>
    <tr><td style="text-align:left; padding:4px;">Diffusion Policy [4], [11]</td><td>✗</td><td>N/A</td><td>78.3</td><td>92.5</td><td>68.3</td><td>50.5</td><td>72.4</td><td>6.5</td></tr>
    <tr><td style="text-align:left; padding:4px;">&pi;<sub>0</sub>-FAST (Paligemma) [2], [19]</td><td>✗</td><td>Custom</td><td>87.0</td><td>63.0</td><td>89.0</td><td>48.0</td><td>71.8</td><td>6.0</td></tr>
    <tr><td style="text-align:left; padding:4px;">SmolVLA (0.24B) [19]</td><td>✗</td><td>Gen Head</td><td>87.0</td><td>93.0</td><td>88.0</td><td>63.0</td><td>82.8</td><td>5.3</td></tr>
    <tr><td style="text-align:left; padding:4px;">SmolVLA (2.25B) [19]</td><td>✗</td><td>Gen Head</td><td>93.0</td><td>94.0</td><td>91.0</td><td>77.0</td><td>88.8</td><td>4.0</td></tr>
    <tr><td style="text-align:left; padding:4px;">OpenVLA-OFT [10]</td><td>✗</td><td>Custom</td><td>94.3</td><td>95.2</td><td>91.7</td><td><u>86.5</u></td><td>91.9</td><td>2.8</td></tr>
    <tr><td style="text-align:left; padding:4px;">&pi;<sub>0.5</sub> - KI [5]</td><td>✗</td><td>Gen Head</td><td>96.6</td><td>97.2</td><td>94.6</td><td>85.8</td><td>93.3</td><td>2.3</td></tr>
    <tr style="font-weight:bold;"><td style="text-align:left; padding:4px;">VLA-0 (Ours)</td><td>✗</td><td>Simple</td><td>97.0</td><td>97.8</td><td>96.2</td><td>87.6</td><td>94.7</td><td>1.0</td></tr>
    <tr style="border-top:1px solid #ccc;"><td colspan="9" style="height:4px;"></td></tr>
    <tr><td style="text-align:left; padding:4px;">Octo [21]</td><td>✓</td><td>Gen Head</td><td>78.9</td><td>85.7</td><td>84.6</td><td>51.1</td><td>75.1</td><td>8.8</td></tr>
    <tr><td style="text-align:left; padding:4px;">OpenVLA [11]</td><td>✓</td><td>Dis. Tok.</td><td>84.7</td><td>88.4</td><td>79.2</td><td>53.7</td><td>76.5</td><td>8.0</td></tr>
    <tr><td style="text-align:left; padding:4px;">&pi;<sub>0</sub>-FAST [16]</td><td>✓</td><td>Custom</td><td>90.0</td><td>86.0</td><td>95.0</td><td>73.0</td><td>86.0</td><td>6.5</td></tr>
    <tr><td style="text-align:left; padding:4px;">Molmo Act [12]</td><td>✓</td><td>Dis. Tok.</td><td>87.0</td><td>95.4</td><td>87.6</td><td>77.2</td><td>86.8</td><td>6.5</td></tr>
    <tr><td style="text-align:left; padding:4px;">GR00T-N1 [1]</td><td>✓</td><td>Gen Head</td><td>94.4</td><td>97.6</td><td>93.0</td><td><u>90.6</u></td><td>93.9</td><td>4.5</td></tr>
    <tr><td style="text-align:left; padding:4px;">&pi;<sub>0</sub> [2]</td><td>✓</td><td>Gen Head</td><td>96.8</td><td><b>98.8</b></td><td>95.8</td><td>85.2</td><td>94.2</td><td>3.3</td></tr>
    <tr><td style="text-align:left; padding:4px;">&pi;<sub>0.5</sub> - KI [5]</td><td>✓</td><td>Gen Head</td><td><b>98.0</b></td><td>97.8</td><td>95.6</td><td>85.8</td><td>94.3</td><td>3.0</td></tr>
    <tr style="font-weight:bold;"><td style="text-align:left; padding:4px;">OpenVLA-OFT [10]</td><td>✓</td><td>Custom</td><td>97.6</td><td>98.4</td><td><b>97.9</b></td><td><b>94.5</b></td><td>97.1</td><td>1.5</td></tr>
    <tr style="color:gray; font-style:italic; border-bottom:2px solid black;">
      <td style="text-align:left; padding:4px;">VLA-0 (Ours)</td><td>✗</td><td>Simple</td><td>97.0</td><td>97.8</td><td>96.2</td><td>87.6</td><td>94.7</td><td>2.8</td>
    </tr>
  </tbody>
</table>

### Results

`At simulation (LIBERO)`,<br>
- No large-scale pre-train : outperforming the second best method by 1.4 points on average.
- With large-scale pre-train : Overall, it gets the second best average rank 2.8, trailing only OpenVLA-OFT [10] (average rank 1.5), a custom VLA model.


`At real-world`,<br>
- We compare with SmolVLA [19], a strong baseline that was specifically trained on the large-scale SO-100 dataset and has been shown to outperform popular methods like π0 [2] and ACT [23] on this platform.

### Ablation studies

<table border="1" style="border-collapse: collapse; width: 100%; text-align: center;">
  <thead>
    <tr style="background-color: #f2f2f2;">
      <th>Row ID</th>
      <th>Ensemble Act.</th>
      <th>Masked Act. Aug.</th>
      <th>Tiled Img.</th>
      <th>Act. Res.</th>
      <th>Avg. Succ.</th>
      <th>&Delta; perf.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>&check;</td>
      <td>&check;</td>
      <td>&check;</td>
      <td>1000</td>
      <td>94.7</td>
      <td>0.0</td>
    </tr>
    <tr>
      <td>1</td>
      <td style="color: red;">&cross;</td>
      <td>&check;</td>
      <td>&check;</td>
      <td>1000</td>
      <td>92.0</td>
      <td>-2.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>&check;</td>
      <td style="color: red;">&cross;</td>
      <td>&check;</td>
      <td>1000</td>
      <td>93.5</td>
      <td>-1.2</td>
    </tr>
    <tr>
      <td>3</td>
      <td>&check;</td>
      <td>&check;</td>
      <td>&check;</td>
      <td>4000</td>
      <td>94.2</td>
      <td>-0.5</td>
    </tr>
    <tr>
      <td>4</td>
      <td>&check;</td>
      <td>&check;</td>
      <td>&check;</td>
      <td>250</td>
      <td>93.2</td>
      <td>-1.5</td>
    </tr>
    <tr>
      <td>5</td>
      <td>&check;</td>
      <td>&check;</td>
      <td style="color: red;">&cross;</td>
      <td>1000</td>
      <td>94.5</td>
      <td>-0.2</td>
    </tr>
  </tbody>
</table>

<br>

## 5. Conculsion

The key strength of the research is zero-modification of VLM.

Without adding extra action tokens or encoder, adopting carefully designed training and inference recipe can achieve SOTA when compared with samely not Largely pre-trained VLAs.

<br>

## Learned

#### Action Vectors

- (dx,dy,dz)
- (droll, dpitch, dyaw)
- (gripper)

<img src="/assets/images/posts/Yaw_Axis.svg" alt="roll, dpitch, dyaw" width="300">

#### Does VLA-0 has general task model?

YES?

#### Personal Thoughts

> 결국 모든 VLA들은 현재 VLM의 이미지 기반 상황인식과 + text input을 동시에 받을 수 있는 지점을 이용한다.
<br>
<br>VLM을 굳이 채택하는 이유는 결국 로봇의 상황판단은 물리세계에 대한 이해를 의미하고, 그 지점에서 `대규모 인터넷 자료를 학습한 pre-trained VLM의 이점`을 사용하는 것이다.
